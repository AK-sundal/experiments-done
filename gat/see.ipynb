{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ModuleList, functional as F, init\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphAttentionLayer(MessagePassing):\n",
    "    def __init__(\n",
    "        self, _in: int, _out: int, heads: int, conct: bool = True, dropout: float = 0.6\n",
    "    ):\n",
    "        super().__init__(aggr=\"add\")\n",
    "        self._in = _in\n",
    "        self._out = _out\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.conct = conct\n",
    "\n",
    "        # Initialize weights for each head\n",
    "        self.weights = ModuleList([Linear(_in, _out, bias=True) for _ in range(heads)])\n",
    "        self.attention = ModuleList(\n",
    "            [Linear(2 * _out, 1, bias=True) for _ in range(heads)]\n",
    "        )\n",
    "        def reset_parameters(self):\n",
    "            init.xavier_uniform_(self.weight)\n",
    "            init.xavier_uniform_(self.a)\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        emb_head = []\n",
    "        # Independently calculate node embeddings for each head\n",
    "        for head in range(self.heads):\n",
    "            x_transf = self.weights[head](x)\n",
    "            emb = self.propagate(edge_index, x=x_transf, head=head)\n",
    "            emb_head.append(emb)\n",
    "\n",
    "        if self.conct:\n",
    "            out = torch.cat(emb_head, dim=-1)  # Concatenate\n",
    "        else:\n",
    "            out = torch.mean(torch.stack(emb_head), dim=0)  # Average\n",
    "        return out  # (N, heads * out_features)\n",
    "\n",
    "    def message(\n",
    "        self, x_j: torch.Tensor, x_i: torch.Tensor, edge_index: torch.Tensor, head: int\n",
    "    ) -> torch.Tensor:\n",
    "        feat = torch.cat([x_j, x_i], dim=-1)  # (E, 2 * F')\n",
    "        e = self.attention[head](feat)  # Attention coefficients\n",
    "        e = F.leaky_relu(e, negative_slope=0.2)\n",
    "        norm_e = softmax(\n",
    "            e, index=edge_index[1], dim=0\n",
    "        )  # Normalize by destination nodes\n",
    "        # Optional dropout on attention weights\n",
    "        # norm_e = F.dropout(norm_e, p=self.dropout, training=self.training)\n",
    "        return norm_e * x_j  # Weighted features\n",
    "\n",
    "    def update(self, aggr_out: torch.Tensor) -> torch.Tensor:\n",
    "        return aggr_out  # No activation in the update for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATTransductive(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.gat_1 = GraphAttentionLayer(in_dim, hid_dim, heads=8, conct=True)\n",
    "        self.gat_2 = GraphAttentionLayer(hid_dim * 8, out_dim, heads=1, conct=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.gat_1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.gat_2(x, edge_index)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Planetoid(root=\"/home/a373k/graph/gat-model/\", name=\"Cora\")\n",
    "# data = dataset[0]  # Load the first graph object in the dataset\n",
    "# model = GATinductive(\n",
    "#     in_dim=data.num_node_features, hid_dim=8, out_dim=dataset.num_classes\n",
    "# )\n",
    "# optimizer = Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(model, data, optimizer):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
    "        accs.append(\n",
    "            int(correct.sum()) / int(mask.sum())\n",
    "        )  # Derive ratio of correct predictions.\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function to perform t-SNE and save the plot\n",
    "def save_tsne_plot(model, data, num_classes, dataset_name):\n",
    "    model.eval()\n",
    "\n",
    "    # Extract features from the first hidden layer of GAT\n",
    "    with torch.no_grad():\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        features = model.gat_1(x, edge_index)  # First GAT layer\n",
    "        features = features.detach().cpu().numpy()  # Convert to numpy for t-SNE\n",
    "\n",
    "    # Apply t-SNE for 2D visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_tsne = tsne.fit_transform(features)\n",
    "\n",
    "    # Get ground truth labels (node classes)\n",
    "    labels = data.y.cpu().numpy()\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(num_classes):\n",
    "        plt.scatter(\n",
    "            features_tsne[labels == i, 0],\n",
    "            features_tsne[labels == i, 1],\n",
    "            label=f\"Class {i}\",\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(f\"t-SNE of GAT First Hidden Layer Features ({dataset_name})\")\n",
    "    save_path = f\"tsne_{dataset_name}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the plot to avoid overlapping issues\n",
    "    print(f\"Saved t-SNE plot for {dataset_name} at {save_path}\")\n",
    "\n",
    "\n",
    "# List of datasets\n",
    "dataset_names = [\"Cora\", \"CiteSeer\", \"PubMed\"]\n",
    "\n",
    "# Loop through each dataset\n",
    "for name in dataset_names:\n",
    "    dataset = Planetoid(root=f\"./{name}_dataset/\", name=name)\n",
    "    data = dataset[0]\n",
    "\n",
    "    # Initialize the model (adjust dimensions as per your implementation)\n",
    "    model = GATTransductive(\n",
    "        in_dim=data.num_node_features, hid_dim=8, out_dim=dataset.num_classes\n",
    "    )\n",
    "\n",
    "    # Train the model (you can use your own training loop here)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    print(f\"{name} results\")\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train(model, data, optimizer)\n",
    "        acc = test(model, data, optimizer)\n",
    "        \n",
    "        print(f\"epoch:{epoch}=> trainLoss:{loss:.4f}, trainAcc:{acc[0]:.3f}, valAcc:{acc[1]:.3f}, testAcc:{acc[2]:.3f}\")\n",
    "\n",
    "    # Save the t-SNE plot for this dataset\n",
    "    save_tsne_plot(model, data, num_classes=dataset.num_classes, dataset_name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATInductive(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # First GAT layer with 4 attention heads and 256 features per head\n",
    "        self.gat_1 = GraphAttentionLayer(\n",
    "            in_dim, hid_dim, heads=4, conct=True\n",
    "        )  # Output: hid_dim * 4\n",
    "        # Second GAT layer with 4 attention heads and 256 features per head\n",
    "        self.gat_2 = GraphAttentionLayer(\n",
    "            hid_dim * 4, hid_dim, heads=4, conct=True\n",
    "        )  # Output: hid_dim * 4\n",
    "        # Third GAT layer with 6 attention heads for multi-label classification\n",
    "        self.gat_3 = GraphAttentionLayer(\n",
    "            hid_dim * 4, out_dim, heads=6, conct=False\n",
    "        )  # Output: out_dim\n",
    "        # Skip connection layers\n",
    "        self.skip_1 = torch.nn.Linear(in_dim, hid_dim * 4)\n",
    "        self.skip_2 = torch.nn.Linear(hid_dim * 4, hid_dim * 4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1 with skip connection\n",
    "        skip_x = self.skip_1(x)\n",
    "        x = F.elu(self.gat_1(x, edge_index)) + skip_x\n",
    "\n",
    "        # Layer 2 with skip connection\n",
    "        skip_x = self.skip_2(x)\n",
    "        x = F.elu(self.gat_2(x, edge_index)) + skip_x\n",
    "\n",
    "        # Layer 3 (output layer with sigmoid activation for multi-label classification)\n",
    "        x = self.gat_3(x, edge_index)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
      "Extracting PPI/train_mask/ppi.zip\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
      "Extracting PPI/val_mask/ppi.zip\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
      "Extracting PPI/test_mask/ppi.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Load the PPI dataset\n",
    "train_dataset = PPI(root=\"PPI/train_mask\", split=\"train\")\n",
    "val_dataset = PPI(root=\"PPI/val_mask\", split=\"val\")\n",
    "test_dataset = PPI(root=\"PPI/test_mask\", split=\"test\")\n",
    "\n",
    "# Create data loaders for mini-batching (batch size = 2 as per the specification)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GATInductive(in_dim=50, hid_dim=64, out_dim=121)\n",
    "# Define optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.005)\n",
    "criterion = torch.nn.BCELoss()  # Binary Cross-Entropy for multi-label classification\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)  # Forward pass\n",
    "        loss = criterion(out, data.y)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "# Validation/Test loop\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# Main training process\n",
    "for epoch in range(1, 101):\n",
    "    train_loss = train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\"\n",
    "    )\n",
    "test_loss = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
